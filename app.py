import os
import sys
import streamlit as st

# ê¸°ì¡´ RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# -----------------------------------------------------------
# 1. API í‚¤ ì„¤ì • (Secrets ì‚¬ìš©)
# -----------------------------------------------------------
try:
    # Streamlit Secretsì—ì„œ API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì˜¤ë¥˜: Google API Keyê°€ Streamlit Secretsì— 'GOOGLE_API_KEY'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # API í‚¤ê°€ ì—†ìœ¼ë©´ ì•±ì„ ê³„ì† ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    st.stop()

# íŒŒì¼ ì„¤ì •: íŒŒì¼ ì´ë¦„ì€ 'rulebook.pdf'ë¡œ ê°€ì • (1ë‹¨ê³„ì—ì„œ ë³€ê²½ ìš”ì²­ë¨)
file_path = "rulebook.pdf" 

# -----------------------------------------------------------
# 2. RAG êµ¬ì„± í•¨ìˆ˜ (ë‹¨ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
# -----------------------------------------------------------
@st.cache_resource
def setup_rag_chain():
    st.write("ğŸ“– ê·œì •ì§‘ì„ ì½ëŠ” ì¤‘...")
    # 1. PDF ë¡œë“œ
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()
    except Exception as e:
        # íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ëª…í™•í•˜ê²Œ í‘œì‹œí•˜ê³ , Noneì„ ë°˜í™˜í•˜ì—¬ ë¡œë”© ì‹¤íŒ¨ë¥¼ ì•Œë¦¼
        st.error(f"ì˜¤ë¥˜: PDF íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHub ì €ì¥ì†Œì˜ ë£¨íŠ¸ì— '{file_path}' íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì—ëŸ¬: {e}")
        return None

    st.write("ğŸ§  AIê°€ ê·œì •ì§‘ì„ í•™ìŠµ ì¤‘ (ì„ë² ë”© ìƒì„±)...")
    # 2. í…ìŠ¤íŠ¸ ë¶„í•  (ìµœì í™”ëœ ì„¤ì • ìœ ì§€)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = text_splitter.split_documents(documents)
    
    # 3. ë²¡í„° DB ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    # 4. ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ìµœì í™”ëœ k=15 ìœ ì§€)
    retriever = vector_db.as_retriever(search_kwargs={"k": 15})
    
    # 5. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1)
    prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì •ë™ê³ ë“±í•™êµ ìƒí™œê·œì • í•´ì„ AIì…ë‹ˆë‹¤.
        ë°˜ë“œì‹œ ê·œì • ì¡°í•­(ì œëª‡ì¡° ëª‡í•­)ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        
        ê·œì • ë¬¸ì¥:
        {context}
        
        ì§ˆë¬¸:
        {question}
        
        ì² ì €íˆ ê·œì • ë¬¸ì„œì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
    """)
    
    # 6. ì²´ì¸ êµ¬ì„±
    chain = (
        RunnableParallel({
            "context": retriever,
            "question": RunnablePassthrough()
        })
        | prompt
        | llm
    )
    st.success("ğŸ‰ ê·œì •ì§‘ í•™ìŠµ ì™„ë£Œ!")
    return chain

# -----------------------------------------------------------
# 3. Streamlit ì•± ì‹¤í–‰ ì˜ì—­
# -----------------------------------------------------------

# ì œëª© ì„¤ì •
st.title("ğŸ« ì •ë™ê³ ë“±í•™êµ í•™ìƒìƒí™œê·œì • AI ë„ìš°ë¯¸")
st.subheader("ê·œì •ì§‘ì„ í•™ìŠµí•œ AIì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")

# RAG ì²´ì¸ ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
rag_chain = None
with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤..."):
    rag_chain = setup_rag_chain()

# rag_chainì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ì•±ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ ì‹¤í–‰
if rag_chain:
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€íšŒ ì…ìƒ ì‹œ ìƒì ì€ ëª‡ ì ì¸ê°€ìš”?)")

    if user_query:
        # ë‹µë³€ ìƒì„± ë° ì¶œë ¥
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                answer = rag_chain.invoke(user_query)
                st.markdown("---")
                st.markdown(f"**ğŸ¤– ë‹µë³€:**")
                st.info(answer.content)
            except Exception as e:
                # LLM í˜¸ì¶œ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ì²˜ë¦¬
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ ë˜ëŠ” LLM ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
