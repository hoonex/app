import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# --- ì„¤ì • ---
file_path = "rulebook.pdf" 
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 150
RETRIEVER_K = 15
GEMINI_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"

# 1. API í‚¤ ì„¤ì • (Secretsì—ì„œ í‚¤ë¥¼ ê°€ì ¸ì™€ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •)
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì˜¤ë¥˜: Google API Keyê°€ Streamlit Secretsì— 'GOOGLE_API_KEY'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop() # í‚¤ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì•± ì‹¤í–‰ ì¤‘ë‹¨

# 2. RAG êµ¬ì„± í•¨ìˆ˜ (ë‹¨ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def setup_rag_chain():
    st.write("ğŸ“– ê·œì •ì§‘ì„ ì½ëŠ” ì¤‘...")
    # 1. PDF ë¡œë“œ
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: PDF íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{file_path}' íŒŒì¼ì´ GitHub ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì—ëŸ¬: {e}")
        return None 

    st.write("ğŸ§  AIê°€ ê·œì •ì§‘ì„ í•™ìŠµ ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = text_splitter.split_documents(documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    retriever = vector_db.as_retriever(search_kwargs={"k": RETRIEVER_K})
    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.1)
    
    prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì •ë™ê³ ë“±í•™êµ ìƒí™œê·œì • í•´ì„ AIì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ê·œì • ì¡°í•­(ì œëª‡ì¡° ëª‡í•­)ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        ê·œì • ë¬¸ì¥: {context}
        ì§ˆë¬¸: {question}
        ì² ì €íˆ ê·œì • ë¬¸ì„œì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
    """)
    
    chain = (
        RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
        | prompt
        | llm
    )
    st.success("ğŸ‰ ê·œì •ì§‘ í•™ìŠµ ì™„ë£Œ!")
    return chain

# 3. Streamlit ì•± ì‹¤í–‰
st.title("ğŸ« ì •ë™ê³ ë“±í•™êµ í•™ìƒìƒí™œê·œì • AI ë„ìš°ë¯¸")
st.subheader("ê·œì •ì§‘ì„ í•™ìŠµí•œ AIì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")

rag_chain = None
with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤..."):
    rag_chain = setup_rag_chain()

if rag_chain:
    user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€íšŒ ì…ìƒ ì‹œ ìƒì ì€ ëª‡ ì ì¸ê°€ìš”?)")

    if user_query:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                answer = rag_chain.invoke(user_query)
                st.markdown("---")
                st.markdown(f"**ğŸ¤– ë‹µë³€:**")
                st.info(answer.content)
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
