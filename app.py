import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

file_path = "rulebook.pdf" 
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100
RETRIEVER_K = 50
GEMINI_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"

try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì˜¤ë¥˜: Google API Keyê°€ Streamlit Secretsì— 'GOOGLE_API_KEY'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def setup_rag_chain():
    st.write("ê·œì •ì§‘ì„ ì½ëŠ” ì¤‘...")
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: PDF íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{file_path}' íŒŒì¼ì´ GitHub ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì—ëŸ¬: {e}")
        return None 

    st.write("AIê°€ ê·œì •ì§‘ì„ í•™ìŠµ ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = text_splitter.split_documents(documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    retriever = vector_db.as_retriever(search_kwargs={"k": RETRIEVER_K})
    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.1)
    
    prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì •ë™ê³ ë“±í•™êµ ìƒí™œê·œì • í•´ì„ AIì…ë‹ˆë‹¤.
    1. ë°˜ë“œì‹œ ì¡°í•­(ì œëª‡ì¡° ëª‡í•­)ê³¼ ë²Œì /ìƒì  ì ìˆ˜('ìˆ«ì')ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. 
    2.ì‚¬ìš©ìê°€ í•œ ì§ˆë¬¸ë“¤ì„ ë§¤ìš° ë„“ê²Œ í¬ê´„ì ìœ¼ë¡œ ìƒê°í•˜ì„¸ìš”.
    3. ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ì¡°í•­(ì˜ˆ: ì´ì¹™, ëª©ì , ì§•ê³„ì˜ ì¼ë°˜ ì›ì¹™ ë“±)ì€ ëŒ€ë¶€ë¶„ ì œì™¸í•˜ê³ , ì˜¤ì§ ì œ112ì¡°ë‚˜ 113ì¡°ì˜ êµ¬ì²´ì ì¸ í•­ëª© ì¤‘ ê´€ë ¨ì„± ë†’ì€ í•­ëª©ë§Œ ìµœëŒ€ 3ê°œë¥¼ ì œì‹œí•˜ì„¸ìš”. 
    ê·œì • ë¬¸ì¥: {context} 
    ì§ˆë¬¸: {question} 
    ì² ì €íˆ ê²€ìƒ‰ëœ ê·œì • ë¬¸ì„œì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
    íŠ¹íˆ ë²Œì  ë° ìƒì ì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì˜ 'í‘œ, ë¦¬ìŠ¤íŠ¸, ëª©ë¡'ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ì ìˆ˜ë¥¼ ì°¾ì•„ ë¹„êµí•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
    """)
    
    chain = (
        RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
        | prompt
        | llm
    )
    st.success("í•™ìŠµ ì™„ë£Œ!")
    return chain

# 3. Streamlit ì•± ì‹¤í–‰
st.title("ğŸ« ì •ë™ê³  í•™ìƒìƒí™œê·œì • ë„ìš°ë¯¸")
st.subheader("ê·œì •ì§‘ì„ í•™ìŠµí•œ ë„ìš°ë¯¸ì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")

rag_chain = None
with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤..."):
    rag_chain = setup_rag_chain()

if rag_chain:
    user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

    if user_query:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                answer = rag_chain.invoke(user_query)
                st.markdown("---")
                st.markdown(f"ë‹µë³€:")
                st.info(answer.content)
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")









