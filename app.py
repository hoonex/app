import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

file_path = "rulebook.pdf" 
CHUNK_SIZE = 2000
CHUNK_OVERLAP = 150
RETRIEVER_K = 75
GEMINI_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"

try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì˜¤ë¥˜: Google API Keyê°€ Streamlit Secretsì— 'GOOGLE_API_KEY'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def setup_rag_chain():
    st.write("ê·œì •ì§‘ì„ ì½ëŠ” ì¤‘...")
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: PDF íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{file_path}' íŒŒì¼ì´ GitHub ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì—ëŸ¬: {e}")
        return None 

    st.write("AIê°€ ê·œì •ì§‘ì„ í•™ìŠµ ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = text_splitter.split_documents(documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    retriever = vector_db.as_retriever(search_kwargs={"k": RETRIEVER_K})
    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.1)
    
    prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì •ë™ê³ ë“±í•™êµ ìƒí™œê·œì • í•´ì„ AIì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ê·œì • ì¡°í•­(ì œëª‡ì¡° ëª‡í•­)ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        ê·œì • ë¬¸ì¥: {context}
        ì§ˆë¬¸: {question}
        ì² ì €íˆ ê·œì • ë¬¸ì„œì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
    """)
    
    chain = (
        RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
        | prompt
        | llm
    )
    st.success("í•™ìŠµ ì™„ë£Œ!")
    return chain

# 3. Streamlit ì•± ì‹¤í–‰
st.title("ğŸ« ì •ë™ê³  í•™ìƒìƒí™œê·œì • ë„ìš°ë¯¸")
st.subheader("ê·œì •ì§‘ì„ í•™ìŠµí•œ ë„ìš°ë¯¸ì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")

rag_chain = None
with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤..."):
    rag_chain = setup_rag_chain()

if rag_chain:
    user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

    if user_query:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                answer = rag_chain.invoke(user_query)
                st.markdown("---")
                st.markdown(f"ë‹µë³€:")
                st.info(answer.content)
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")


